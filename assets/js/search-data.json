{
  
    
        "post0": {
            "title": "Notebook Blog Post",
            "content": "About . This notebook is a demonstration of Topic Modeling of Natural Language Processing. . Natural language processing is the analysis of languages used in the system that exists in the nltk library, where this is analyzed to cut, extract, and transform to new data so that we may gain useful insights. It only employs the languages that are included in the library because NLP-related items are present there, and it can&#39;t understand anything else. . The recognition of words from the themes included in a document or a corpus of data is known as topic modeling. This is useful since retrieving words from a document takes significantly longer and is far more complicated than extracting them from the document&#39;s themes. We&#39;re forced to look into unsupervised techniques, such as Topic Modeling, to extract these subjects because I don&#39;t have any categories or labels. . Although topic models like LDA and NMF have proven to be useful beginning points, We&#39;ve always found that creating significant subjects required a lot of hyperparameter adjustment. . Furthermore, I wanted to leverage transformer-based models like BERT, which have shown remarkable success in a variety of NLP applications in recent years. Pre-trained models are especially useful because they are believed to have more accurate word and phrase representations. . I recently came across a fantastic project called Top2Vec, which used document and word embeddings to produce clearly understandable subjects. I started looking at the code to see if there was a way to make Top2Vec work with pre-trained transformer models. . Doc2Vec has a significant advantage in that the resulting document- and word embeddings are concurrently embedded in the same space, allowing document embeddings to be represented by neighboring word embeddings. Unfortunately, because BERT embeddings are token-based and do not always occupy the same space, this proved challenging. . Although they might share the same space, due to the contextual nature of BERT, the resulting size of the word embeddings is relatively big. Furthermore, the quality of the generated sentence- or document embeddings may deteriorate. . Instead, I devised an approach that could make use of BERT and transformer embeddings. The outcome is BERTopic, a topic generation algorithm based on state-of-the-art embeddings. . The main focus of this post will be a tutorial on how to use BERT to develop your own topic model rather than the use of BERTopic. . Data &amp; Packages . For this, we use the readstech dataset which contains 1000 data. . The Pandas DataFrame is a structure that holds two-dimensional data as well as the labels that go with it. DataFrames are frequently utilized in domains such as data science, machine learning, scientific computing, and many others that deal with large amounts of data. For the purpose of pre-processing the data pandas dataframe will be used. . import pandas as pd . . NLTK is a Python toolbox for working with natural language processing. It supplies us with a variety of text processing libraries as well as a large number of test datasets. Tokenizing, parse tree visualization, and other operations can be accomplished with NLTK. . import nltk # Downloaing resource nltk.download(&#39;wordnet&#39;) nltk.download(&#39;stopwords&#39;) nltk.download(&#39;punkt&#39;) . . [nltk_data] Downloading package wordnet to /root/nltk_data... [nltk_data] Unzipping corpora/wordnet.zip. [nltk_data] Downloading package stopwords to /root/nltk_data... [nltk_data] Unzipping corpora/stopwords.zip. [nltk_data] Downloading package punkt to /root/nltk_data... [nltk_data] Unzipping tokenizers/punkt.zip. . True . Convert a JSON string to pandas object. . df = pd.read_json(&#39;_search_1000.json&#39;, orient=&#39;str&#39;) . . Normalize semi-structured JSON data into a flat table. . works_data = pd.json_normalize(data=df.hits.hits) works_data.head(5) . _index _type _id _score _ignored _source.name _source.id _source.URL _source.sourceURL _source.version ... _source.bulk.doi _source.bulk.pii _source.bulk.pmc _source.bulk.language _source.bulk.reference _source.bulk.country _source.authors _source.description _source.tags _source.bulk.day . 0 reads_rex_ncbi_pubmed | _doc | 15574944 | 1.0 | [description.keyword] | Hypothesis for the role of nutrient starvation... | 15574944 | [https://pubmed.ncbi.nlm.nih.gov/15574944] | [https://pubmed.ncbi.nlm.nih.gov/15574944] | 0.1 | ... | 10.1128/AEM.70.12.7418-7425.2004 | 70/12/7418 | PMC535154 | eng | [Biotechnol Bioeng. 1991 Aug 20;38(5):499-506,... | United States | [Hunt, Stephen M, Werner, Erin M, Huang, Baoch... | A combination of experimental and theoretical ... | [Bacterial Adhesion, Biofilms, Bioreactors, Co... | NaN | . 1 reads_rex_ncbi_pubmed | _doc | 15574945 | 1.0 | [description.keyword] | Shear rate moderates community diversity in fr... | 15574945 | [https://pubmed.ncbi.nlm.nih.gov/15574945] | [https://pubmed.ncbi.nlm.nih.gov/15574945] | 0.1 | ... | 10.1128/AEM.70.12.7426-7435.2004 | 70/12/7426 | PMC535146 | eng | [Annu Rev Microbiol. 2002;56:187-209, Can J Mi... | United States | [Rickard, Alexander H, McBain, Andrew J, Stead... | The development of freshwater multispecies bio... | [Bacteria, Bacterial Adhesion, Biofilms, Cultu... | NaN | . 2 reads_rex_ncbi_pubmed | _doc | 15574964 | 1.0 | [description.keyword] | Diversity of nontuberculoid Mycobacterium spec... | 15574964 | [https://pubmed.ncbi.nlm.nih.gov/15574964] | [https://pubmed.ncbi.nlm.nih.gov/15574964] | 0.1 | ... | 10.1128/AEM.70.12.7571-7573.2004 | 70/12/7571 | PMC535200 | eng | [Int J Syst Evol Microbiol. 2000 Mar;50 Pt 2:5... | United States | [September, S M, Brözel, V S, Venter, S N] | Nontuberculous mycobacteria (NTM) are ubiquito... | [Biofilms, Colony Count, Microbial, DNA, Ribos... | NaN | . 3 reads_rex_ncbi_pubmed | _doc | 15575282 | 1.0 | [description.keyword] | Three-dimensional dual-morphotype species mode... | 15575282 | [https://pubmed.ncbi.nlm.nih.gov/15575282] | [https://pubmed.ncbi.nlm.nih.gov/15575282] | 0.1 | ... | 10.1021/es049659l | NaN | NaN | eng | NaN | United States | [Martins, António M P, Picioreanu, Cristian, H... | An individual-based model, originally develope... | [Bacteria, Aerobic, Biofilms, Biomass, Diffusi... | 01 | . 4 reads_rex_ncbi_pubmed | _doc | 15575706 | 1.0 | [description.keyword] | Growth model and metabolic activity of brewing... | 15575706 | [https://pubmed.ncbi.nlm.nih.gov/15575706] | [https://pubmed.ncbi.nlm.nih.gov/15575706] | 0.1 | ... | 10.1021/bp049766j | NaN | NaN | eng | NaN | United States | [Brányik, Tomás, Vicente, António A, Kuncová, ... | In the continuous systems, such as continuous ... | [Bacterial Adhesion, Beer, Biofilms, Bioreacto... | NaN | . 5 rows × 25 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Filtering the data table. Concatenating two columns in one column. . df_new = works_data.filter([&#39;_source.name&#39;,&#39;_source.description&#39;], axis=1) df_new[&quot;concatenate&quot;] = df_new[&quot;_source.name&quot;] + df_new[&quot;_source.description&quot;] df_new . _source.name _source.description concatenate . 0 Hypothesis for the role of nutrient starvation... | A combination of experimental and theoretical ... | Hypothesis for the role of nutrient starvation... | . 1 Shear rate moderates community diversity in fr... | The development of freshwater multispecies bio... | Shear rate moderates community diversity in fr... | . 2 Diversity of nontuberculoid Mycobacterium spec... | Nontuberculous mycobacteria (NTM) are ubiquito... | Diversity of nontuberculoid Mycobacterium spec... | . 3 Three-dimensional dual-morphotype species mode... | An individual-based model, originally develope... | Three-dimensional dual-morphotype species mode... | . 4 Growth model and metabolic activity of brewing... | In the continuous systems, such as continuous ... | Growth model and metabolic activity of brewing... | . ... ... | ... | ... | . 995 Development of real-time in vivo imaging of de... | Staphylococcus epidermidis is the leading caus... | Development of real-time in vivo imaging of de... | . 996 Control of bacterial biofilms with marine alka... | Bacterial biofilms are defined as a community ... | Control of bacterial biofilms with marine alka... | . 997 A new vision for wound healing. | | A new vision for wound healing. | . 998 A study of biofilm-based wound management in s... | Bacterial biofilms cause or complicate numerou... | A study of biofilm-based wound management in s... | . 999 Response of microbial growth to orthophosphate... | Consequences of orthophosphate addition for co... | Response of microbial growth to orthophosphate... | . 1000 rows × 3 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; We are further filtering the data by dropping the columns which is not necessary. . desc_df = df_new.copy() ds = desc_df.drop(columns=[&#39;_source.name&#39;, &#39;_source.description&#39;]) ds . concatenate . 0 Hypothesis for the role of nutrient starvation... | . 1 Shear rate moderates community diversity in fr... | . 2 Diversity of nontuberculoid Mycobacterium spec... | . 3 Three-dimensional dual-morphotype species mode... | . 4 Growth model and metabolic activity of brewing... | . ... ... | . 995 Development of real-time in vivo imaging of de... | . 996 Control of bacterial biofilms with marine alka... | . 997 A new vision for wound healing. | . 998 A study of biofilm-based wound management in s... | . 999 Response of microbial growth to orthophosphate... | . 1000 rows × 1 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; The grouping of distinct variants of the same word is known as lemmatization. Lemmatization in search queries allows users to query any variant of a base term and receive relevant responses. Here we have lemmatized our data. . w_tokenizer = nltk.tokenize.WhitespaceTokenizer() lemmatizer = nltk.stem.WordNetLemmatizer() def lemmatize_text(text): return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)] ds[&#39;text_lemmatized&#39;] = ds.concatenate.apply(lemmatize_text) ds[&quot;lemmatized&quot;] = ds[&#39;text_lemmatized&#39;].agg(lambda x: &#39; &#39;.join(map(str, x))) ds.head(5) . concatenate text_lemmatized lemmatized . 0 Hypothesis for the role of nutrient starvation... | [Hypothesis, for, the, role, of, nutrient, sta... | Hypothesis for the role of nutrient starvation... | . 1 Shear rate moderates community diversity in fr... | [Shear, rate, moderate, community, diversity, ... | Shear rate moderate community diversity in fre... | . 2 Diversity of nontuberculoid Mycobacterium spec... | [Diversity, of, nontuberculoid, Mycobacterium,... | Diversity of nontuberculoid Mycobacterium spec... | . 3 Three-dimensional dual-morphotype species mode... | [Three-dimensional, dual-morphotype, specie, m... | Three-dimensional dual-morphotype specie model... | . 4 Growth model and metabolic activity of brewing... | [Growth, model, and, metabolic, activity, of, ... | Growth model and metabolic activity of brewing... | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; In any natural language, stopwords are the most prevalent words. These stopwords might not contribute much value to the meaning of the document for the purposes of evaluating text data and constructing NLP models. We have removed stopwords from our dataset. . concatenate text_lemmatized lemmatized stop_words . 0 Hypothesis for the role of nutrient starvation... | [Hypothesis, for, the, role, of, nutrient, sta... | Hypothesis for the role of nutrient starvation... | Hypothesis role nutrient starvation biofilm de... | . 1 Shear rate moderates community diversity in fr... | [Shear, rate, moderate, community, diversity, ... | Shear rate moderate community diversity in fre... | Shear rate moderate community diversity freshw... | . 2 Diversity of nontuberculoid Mycobacterium spec... | [Diversity, of, nontuberculoid, Mycobacterium,... | Diversity of nontuberculoid Mycobacterium spec... | Diversity nontuberculoid Mycobacterium specie ... | . 3 Three-dimensional dual-morphotype species mode... | [Three-dimensional, dual-morphotype, specie, m... | Three-dimensional dual-morphotype specie model... | Three-dimensional dual-morphotype specie model... | . 4 Growth model and metabolic activity of brewing... | [Growth, model, and, metabolic, activity, of, ... | Growth model and metabolic activity of brewing... | Growth model metabolic activity brewing yeast ... | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Returning a list of values for further demonstration. . desc = ds[&#39;stop_words&#39;].to_list() desc[0] . . &#39;Hypothesis role nutrient starvation biofilm detachment.A combination experimental theoretical approach wa used investigate role nutrient starvation potential trigger biofilm detachment. Experimental observation detachment variety biofilm system made pure culture Pseudomonas aeruginosa. These observation indicated biofilms grown continuous-flow condition detached flow wa stopped, hollow cell cluster sometimes observed biofilms grown flow cells, lysed cell apparent internal stratum colony biofilms. When biofilms nutrient starved continuous-flow conditions, detachment still occurred, suggesting starvation accumulation metabolic product wa responsible triggering detachment particular system. A cellular automaton computer model biofilm dynamic wa used explore starvation-dependent detachment mechanism. The model predicted biofilm structure dynamic qualitatively similar observed experimentally. The predicted feature included centrally located void appearing sufficiently large cell clusters, gradient growth rate within clusters, release biofilm simulated stopped-flow conditions. The model wa also able predict biofilm sloughing resulting solely detachment mechanism. These result support conjecture nutrient starvation environmental cue release microbe biofilm.&#39; . Embeddings . Converting the documents to numerical data is the first step we must take. We utilize BERT for this since it extracts multiple embeddings depending on the word&#39;s context. Not only that, but there are a slew of pre-trained models ready to go. It&#39;s up to you how you generate BERT embeddings for a document. However, I prefer to utilize the sentence-transformers package because the resulting embeddings have consistently proven to be of excellent quality and function well for document-level embeddings. Before generating the document embeddings, install the package with pip install sentence-transformers. . !pip install sentence_transformers from sentence_transformers import SentenceTransformer model = SentenceTransformer(&#39;distilbert-base-nli-mean-tokens&#39;) embeddings = model.encode(desc, show_progress_bar=True) . . Collecting sentence_transformers Downloading sentence-transformers-2.2.0.tar.gz (79 kB) |████████████████████████████████| 79 kB 4.0 MB/s Collecting transformers&lt;5.0.0,&gt;=4.6.0 Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB) |████████████████████████████████| 4.0 MB 30.1 MB/s Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.63.0) Requirement already satisfied: torch&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.10.0+cu111) Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.11.1+cu111) Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.21.5) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.0.2) Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.4.1) Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.2.5) Collecting sentencepiece Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB) |████████████████████████████████| 1.2 MB 42.0 MB/s Collecting huggingface-hub Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB) |████████████████████████████████| 77 kB 6.4 MB/s Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch&gt;=1.6.0-&gt;sentence_transformers) (3.10.0.2) Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers&lt;5.0.0,&gt;=4.6.0-&gt;sentence_transformers) (3.6.0) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers&lt;5.0.0,&gt;=4.6.0-&gt;sentence_transformers) (2.23.0) Requirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers&lt;5.0.0,&gt;=4.6.0-&gt;sentence_transformers) (21.3) Collecting tokenizers!=0.11.3,&lt;0.13,&gt;=0.11.1 Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB) |████████████████████████████████| 6.5 MB 36.6 MB/s Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers&lt;5.0.0,&gt;=4.6.0-&gt;sentence_transformers) (4.11.3) Collecting sacremoses Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB) |████████████████████████████████| 895 kB 45.4 MB/s Collecting pyyaml&gt;=5.1 Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB) |████████████████████████████████| 596 kB 48.7 MB/s Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers&lt;5.0.0,&gt;=4.6.0-&gt;sentence_transformers) (2019.12.20) Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging&gt;=20.0-&gt;transformers&lt;5.0.0,&gt;=4.6.0-&gt;sentence_transformers) (3.0.7) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;transformers&lt;5.0.0,&gt;=4.6.0-&gt;sentence_transformers) (3.7.0) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk-&gt;sentence_transformers) (1.15.0) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers&lt;5.0.0,&gt;=4.6.0-&gt;sentence_transformers) (3.0.4) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers&lt;5.0.0,&gt;=4.6.0-&gt;sentence_transformers) (1.24.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers&lt;5.0.0,&gt;=4.6.0-&gt;sentence_transformers) (2.10) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers&lt;5.0.0,&gt;=4.6.0-&gt;sentence_transformers) (2021.10.8) Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers&lt;5.0.0,&gt;=4.6.0-&gt;sentence_transformers) (1.1.0) Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers&lt;5.0.0,&gt;=4.6.0-&gt;sentence_transformers) (7.1.2) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;sentence_transformers) (3.1.0) Requirement already satisfied: pillow!=8.3.0,&gt;=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision-&gt;sentence_transformers) (7.1.2) Building wheels for collected packages: sentence-transformers Building wheel for sentence-transformers (setup.py) ... done Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120747 sha256=31d12e87d0c161b58eead56362d6e1c0123ed46a73bf7e5a5ef7dcb575582840 Stored in directory: /root/.cache/pip/wheels/83/c0/df/b6873ab7aac3f2465aa9144b6b4c41c4391cfecc027c8b07e7 Successfully built sentence-transformers Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece, sentence-transformers Attempting uninstall: pyyaml Found existing installation: PyYAML 3.13 Uninstalling PyYAML-3.13: Successfully uninstalled PyYAML-3.13 Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 sentence-transformers-2.2.0 sentencepiece-0.1.96 tokenizers-0.11.6 transformers-4.18.0 . Clustering . UMAP . UMAP is arguably the best performing of the few dimensionality reduction algorithms since it preserves a large percentage of the high-dimensional local structure in lower dimensionality. Pip can be used to install a package. Before we lower the document embeddings&#39; dimensionality, we need to install umap-learn. We keep the local neighborhood size at 3 and reduce the dimensionality to 2. You can experiment with these parameters to find the best setting for your topic. It&#39;s worth noting that a low dimensionality causes information loss, whilst a large dimensionality causes poor clustering results. . !pip install umap-learn import umap umap_embeddings = umap.UMAP(n_neighbors=3, n_components=2, metric=&#39;cosine&#39;).fit_transform(embeddings) . . Collecting umap-learn Downloading umap-learn-0.5.2.tar.gz (86 kB) |████████████████████████████████| 86 kB 3.1 MB/s Requirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.21.5) Requirement already satisfied: scikit-learn&gt;=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.0.2) Requirement already satisfied: scipy&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.4.1) Requirement already satisfied: numba&gt;=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.51.2) Collecting pynndescent&gt;=0.5 Downloading pynndescent-0.5.6.tar.gz (1.1 MB) |████████████████████████████████| 1.1 MB 34.5 MB/s Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from umap-learn) (4.63.0) Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba&gt;=0.49-&gt;umap-learn) (57.4.0) Requirement already satisfied: llvmlite&lt;0.35,&gt;=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba&gt;=0.49-&gt;umap-learn) (0.34.0) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent&gt;=0.5-&gt;umap-learn) (1.1.0) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn&gt;=0.22-&gt;umap-learn) (3.1.0) Building wheels for collected packages: umap-learn, pynndescent Building wheel for umap-learn (setup.py) ... done Created wheel for umap-learn: filename=umap_learn-0.5.2-py3-none-any.whl size=82708 sha256=4e1b8aeb498ece8b19a3afde43ab22f936b9a495ca9c28c1bdfeb36f66e7eb6d Stored in directory: /root/.cache/pip/wheels/84/1b/c6/aaf68a748122632967cef4dffef68224eb16798b6793257d82 Building wheel for pynndescent (setup.py) ... done Created wheel for pynndescent: filename=pynndescent-0.5.6-py3-none-any.whl size=53943 sha256=a4aee55121ba70a786fcb3001694d83e69fe5a242af57e8071fe3cfb204a451c Stored in directory: /root/.cache/pip/wheels/03/f1/56/f80d72741e400345b5a5b50ec3d929aca581bf45e0225d5c50 Successfully built umap-learn pynndescent Installing collected packages: pynndescent, umap-learn Successfully installed pynndescent-0.5.6 umap-learn-0.5.2 . /usr/local/lib/python3.7/dist-packages/numba/np/ufunc/parallel.py:363: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION &gt;= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled. warnings.warn(problem) . HDBSCAN . After reducing the dimensionality of the document embeddings to 2, we may use HDBSCAN to cluster the documents. Because UMAP maintains a lot of local structure even in lower-dimensional space, HDBSCAN is a density-based method that works well with UMAP. Furthermore, because outliers are considered outliers, HDBSCAN does not coerce data points into clusters. . !pip install hdbscan import hdbscan cluster = hdbscan.HDBSCAN(min_cluster_size=3, metric=&#39;euclidean&#39;, cluster_selection_method=&#39;eom&#39;).fit(umap_embeddings) . . Collecting hdbscan Downloading hdbscan-0.8.28.tar.gz (5.2 MB) |████████████████████████████████| 5.2 MB 5.3 MB/s Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done Requirement already satisfied: joblib&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from hdbscan) (1.1.0) Requirement already satisfied: scipy&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from hdbscan) (1.4.1) Requirement already satisfied: cython&gt;=0.27 in /usr/local/lib/python3.7/dist-packages (from hdbscan) (0.29.28) Requirement already satisfied: numpy&gt;=1.20 in /usr/local/lib/python3.7/dist-packages (from hdbscan) (1.21.5) Requirement already satisfied: scikit-learn&gt;=0.20 in /usr/local/lib/python3.7/dist-packages (from hdbscan) (1.0.2) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn&gt;=0.20-&gt;hdbscan) (3.1.0) Building wheels for collected packages: hdbscan Building wheel for hdbscan (PEP 517) ... done Created wheel for hdbscan: filename=hdbscan-0.8.28-cp37-cp37m-linux_x86_64.whl size=2330779 sha256=d8c76ea558cb81cc0b2f13c9148d6a571748ef5f67a580d2938269047a4c6e16 Stored in directory: /root/.cache/pip/wheels/6e/7a/5e/259ccc841c085fc41b99ef4a71e896b62f5161f2bc8a14c97a Successfully built hdbscan Installing collected packages: hdbscan Successfully installed hdbscan-0.8.28 . We&#39;ve now grouped similar documents into groups that should represent the themes they cover. We can further reduce the dimensionality to 2 and show the resulting clusters as grey dots to visualize the outliers. . import matplotlib.pyplot as plt # Prepare data umap_data = umap.UMAP(n_neighbors=3, n_components=2, min_dist=0.01, metric=&#39;cosine&#39;).fit_transform(embeddings) result = pd.DataFrame(umap_data, columns=[&#39;x&#39;, &#39;y&#39;]) result[&#39;labels&#39;] = cluster.labels_ # Visualize clusters fig, ax = plt.subplots(figsize=(20, 10)) outliers = result.loc[result.labels == -1, :] clustered = result.loc[result.labels != -1, :] plt.scatter(outliers.x, outliers.y, color=&#39;#BDBDBD&#39;, s=0.05) plt.scatter(clustered.x, clustered.y, c=clustered.labels, s=1.9, cmap=&#39;hsv_r&#39;) plt.colorbar() . . &lt;matplotlib.colorbar.Colorbar at 0x7fd012fd4790&gt; . Topic Creation . To address this, I devised a class-based form of TF-IDF (c-TF-IDF), which allows me to extract what distinguishes each set of documents from the others. The following is the method&#39;s intuition. When you use TF-IDF to compare the value of words in a set of papers, you are essentially comparing the relevance of words between documents. What if we treated all documents in a single category (for example, a cluster) as a single document and then applied TF-IDF on it? As a result, each category would have a very long document, and the TF-IDF score would show the most essential terms in the topic. . c-TF-IDF . We must first construct a single document for each cluster of documents in order to create this class-based TF-IDF score. . docs_df = pd.DataFrame(desc, columns=[&quot;stop_words&quot;]) docs_df[&#39;Topic&#39;] = cluster.labels_ docs_df[&#39;Doc_ID&#39;] = range(len(docs_df)) docs_per_topic = docs_df.groupby([&#39;Topic&#39;], as_index = False).agg({&#39;stop_words&#39;: &#39; &#39;.join}) . As a next step, we implement the TF-IDF. . import numpy as np from sklearn.feature_extraction.text import CountVectorizer def c_tf_idf(documents, m, ngram_range=(1, 1)): count = CountVectorizer(ngram_range=ngram_range, stop_words=&quot;english&quot;).fit(documents) t = count.transform(documents).toarray() w = t.sum(axis=1) tf = np.divide(t.T, w) sum_t = t.sum(axis=0) idf = np.log(np.divide(m, sum_t)).reshape(-1, 1) tf_idf = np.multiply(tf, idf) return tf_idf, count tf_idf, count = c_tf_idf(docs_per_topic.stop_words.values, m=len(desc)) . Topic Representation . We take the top 5 words each topic based on their c-TF-IDF scores to construct a topic representation. Because the score is a proxy for information density, the higher the score, the more representative it should be of its topic. . def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=5): words = count.get_feature_names() labels = list(docs_per_topic.Topic) tf_idf_transposed = tf_idf.T indices = tf_idf_transposed.argsort()[:, -n:] top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)} return top_n_words def extract_topic_sizes(df): topic_sizes = (df.groupby([&#39;Topic&#39;]) .stop_words .count() .reset_index() .rename({&quot;Topic&quot;: &quot;Topic&quot;, &quot;stop_words&quot;: &quot;Size&quot;}, axis=&#39;columns&#39;) .sort_values(&quot;Size&quot;, ascending=False)) return topic_sizes top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=5) topic_sizes = extract_topic_sizes(docs_df); topic_sizes.head(5) . /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead. warnings.warn(msg, category=FutureWarning) . Topic Size . 0 -1 | 218 | . 28 27 | 19 | . 80 79 | 19 | . 102 101 | 18 | . 29 28 | 15 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; All documents with no designated themes are referred to as topic name-1. The beauty of HDBSCAN is that it does not force all papers into a single cluster. It is simply an outlier if no cluster could be discovered. . top_n_words[-1][:5] . [(&#39;reactor&#39;, 0.004807089197183047), (&#39;isolates&#39;, 0.004365701493881515), (&#39;gene&#39;, 0.004315372969837364), (&#39;milk&#39;, 0.004180708294424944), (&#39;growth&#39;, 0.00407561161959677)] . To view the words belonging to those topics, we can simply use the dictionary top_n_words to access these topics: . top_n_words[3][:5] . [(&#39;chitosan&#39;, 0.034230436479573176), (&#39;rhamnolipid&#39;, 0.031284359498597175), (&#39;microcolonies&#39;, 0.029662156233845275), (&#39;rhla&#39;, 0.029123315805051703), (&#39;seeding&#39;, 0.02748565694858995)] . Topic Reduction . There&#39;s a chance that, depending on the dataset, we&#39;ll end up with hundreds of new to[ics. We can use HDBSCAN&#39;s min_cluster_size parameter to reduce the amount of topics we get, but it does not allow us to define the exact number of clusters. Top2Vec used a clever approach to minimize the number of subjects by merging the topic vectors that were the most similar to one another. We can use a similar strategy to update the representation of our topics by comparing the c-TF-IDF vectors among topics, merging the most similar ones, and then recalculating the c-TF-IDF vectors: . from sklearn.metrics.pairwise import cosine_similarity for i in range(20): # Calculate cosine similarity similarities = cosine_similarity(tf_idf.T) np.fill_diagonal(similarities, 0) # Extract label to merge into and from where topic_sizes = docs_df.groupby([&#39;Topic&#39;]).count().sort_values(&quot;stop_words&quot;, ascending=False).reset_index() topic_to_merge = topic_sizes.iloc[-1].Topic topic_to_merge_into = np.argmax(similarities[topic_to_merge + 1]) - 1 # Adjust topics docs_df.loc[docs_df.Topic == topic_to_merge, &quot;Topic&quot;] = topic_to_merge_into old_topics = docs_df.sort_values(&quot;Topic&quot;).Topic.unique() map_topics = {old_topic: index - 1 for index, old_topic in enumerate(old_topics)} docs_df.Topic = docs_df.Topic.map(map_topics) docs_per_topic = docs_df.groupby([&#39;Topic&#39;], as_index = False).agg({&#39;stop_words&#39;: &#39; &#39;.join}) # Calculate new topic words m = len(desc) tf_idf, count = c_tf_idf(docs_per_topic.stop_words.values, m) top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=10) topic_sizes = extract_topic_sizes(docs_df); topic_sizes.head(5) . /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead. warnings.warn(msg, category=FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead. warnings.warn(msg, category=FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead. warnings.warn(msg, category=FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead. warnings.warn(msg, category=FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead. warnings.warn(msg, category=FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead. warnings.warn(msg, category=FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead. warnings.warn(msg, category=FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead. warnings.warn(msg, category=FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead. warnings.warn(msg, category=FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead. warnings.warn(msg, category=FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead. warnings.warn(msg, category=FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead. warnings.warn(msg, category=FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead. warnings.warn(msg, category=FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead. warnings.warn(msg, category=FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead. warnings.warn(msg, category=FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead. warnings.warn(msg, category=FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead. warnings.warn(msg, category=FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead. warnings.warn(msg, category=FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead. warnings.warn(msg, category=FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead. warnings.warn(msg, category=FutureWarning) . Topic Size . 0 -1 | 254 | . 24 23 | 25 | . 72 71 | 21 | . 67 66 | 19 | . 23 22 | 19 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; . top_n_words[-1][:5] . [(&#39;reactor&#39;, 0.004308422316201523), (&#39;gene&#39;, 0.004166583485603502), (&#39;growth&#39;, 0.0041528031980485255), (&#39;response&#39;, 0.004128494039649857), (&#39;isolates&#39;, 0.004051489331700905)] . top_n_words[3][:5] . [(&#39;oral&#39;, 0.07577788028092265), (&#39;pellicle&#39;, 0.06690219733734386), (&#39;alloy&#39;, 0.05625604997318628), (&#39;enzyme&#39;, 0.04885593954977157), (&#39;corrosion&#39;, 0.04705523155575453)] . After this, we took the least common topic and merged it with the most similar topic. By repeating this, we can reduce the number of topics. .",
            "url": "https://readstech.github.io/bicbioeng-lab-blog/jupyter/2022/04/10/topic-modeling.html",
            "relUrl": "/jupyter/2022/04/10/topic-modeling.html",
            "date": " • Apr 10, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://readstech.github.io/bicbioeng-lab-blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://readstech.github.io/bicbioeng-lab-blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://readstech.github.io/bicbioeng-lab-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://readstech.github.io/bicbioeng-lab-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}